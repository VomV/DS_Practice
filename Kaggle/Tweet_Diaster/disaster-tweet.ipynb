{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Disaster tweet binary classification","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report, confusion_matrix\n\nimport xgboost as xgb\nimport spacy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Read the input files\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n#Training data\npath = \"/kaggle/input/nlp-getting-started/\"\ntrain_data = pd.read_csv(os.path.join(path, 'train.csv'))\ntest_data = pd.read_csv(os.path.join(path, 'test.csv'))\n\ntrain_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **EDA**","metadata":{}},{"cell_type":"code","source":"train_data.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#EDA\n\ntrain_data = train_data.drop_duplicates(subset=['text', 'target'], keep='first')\ntrain_data.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Tweet char lengths in train and test\nplt.hist(train_data.text.str.len(), bins=20, label='train')\nplt.show()\n\nplt.hist(test_data.text.str.len(), bins=20, label='test', alpha=0.5)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Distribution of the target variable\n\nplt.hist(train_data[\"target\"])\nplt.xticks([0, 1])\nplt.show() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Keywords column\n\nplt.figure(figsize=(20,10))\nsns.barplot(y = train_data['keyword'].value_counts()[:30].index,\n            x = train_data['keyword'].value_counts()[:30],\n            orient='h')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Non disaster tweet keywords\ndf = train_data[train_data.target==0]['keyword'].value_counts()\n\ndf.head(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[(train_data.target==0) & (train_data.keyword=='explode')].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data[(train_data.target==0) & (train_data.keyword=='deluge')].head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##Non diaster keywords also look similar to diaster keywords even though the tweet might be begnign.\n\n##So this wont be a useful feature for the model and might lead to false positives.\n\n##Dropping id, keyword and location (due to large no of NaNs)","metadata":{}},{"cell_type":"code","source":"train_x = train_data[['text']].copy()\ntrain_y = train_data['target'].copy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Text cleaning and extraction\n# - change to lower case\n# - remove html\n# - extract hashtags\n# - remove special char\n# - remove numbers/words with numbers\n# - remove stop words\n# - stemming\n# - lemmatization\n\n#Stopwords\nstopw = set(stopwords.words('english'))\n#Stopwords: keep negations\nstopw = stopw.difference(\n        [\"won't\", \"aren't\", 'nor', 'not', 'no', \"isn't\", \"couldn't\", \"hasn't\", \"hadn't\", \"haven't\",\n         \"didn't\", \"doesn't\", \"wouldn't\", \"can't\"])\n    \ndef text_process(text, stopw=None):\n    \n    text = text.lower()\n    \n    #remove html\n    text = re.sub(r'http[s]\\S+|www\\S+', '', text)\n    \n    #remove numbers and punctuation\n    text = re.sub(r'[^A-Za-z\\s]+', '', text)\n    \n    #Stopwords removal\n    text_list = [word for word in text.split() if word not in stopw]\n    \n    #Stemming the words\n    port_stem = PorterStemmer()\n    text_list = [port_stem.stem(word) for word in text_list]\n                \n    ## Lemmatisation to get the root word\n    lem = WordNetLemmatizer()\n    text_list = [lem.lemmatize(word) for word in text_list]\n\n    ## back to string from list\n    text = \" \".join(text_list)\n    return text\n\ntrain_x['text'] = train_x['text'].apply(lambda x: text_process(x, stopw))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Create numerical features based on word counts in the tweet**","metadata":{}},{"cell_type":"code","source":"class ExtraAttributes(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, add_frac_nonstop=True, add_avg_word_len=True, stopw=stopw):\n        self.add_frac_nonstop = add_frac_nonstop\n        self.add_avg_word_len = add_avg_word_len\n        self.stopw = stopw\n\n\n    def fit(self, X, y=None):\n        return self\n  \n    def transform(self, X, y=None):\n#         print(X.info())\n        #Fraction of non stopwords\n        X['frac_nonstop'] = X['text'].apply(\n                lambda x: len([t for t in x.split() if t not in stopw])/len(x))\n\n        #Average word length\n        X['avg_word_len'] = X['text'].apply(\n            lambda x: np.mean([len(t) for t in x.split() if t not in stopw]))\n        X = X.drop(['text'], axis=1)\n        return np.c_[X]\n                                                ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ExtraAttributes().fit_transform(train_x).shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Using Spacy's pretrained large word 2 vec model for creating word embeddings**","metadata":{}},{"cell_type":"markdown","source":"Other approches can be explored as well:\n\n - TFIDF (simpler approach of using normalized word frequencies, context of the tweet might not be captured)\n - Word embedding approaches like Word2Vec, Glove and BERT preserve the context and hence perform better","metadata":{}},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_lg\")\n\nclass w2v_spacy(BaseEstimator, TransformerMixin):\n    def __init__(self, nlp):\n        self.nlp = nlp\n        self.dim = 300\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n#         print(X.shape)\n        return np.c_[[self.nlp(text).vector for text in X['text']]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Validation set stratified with target variable\n\nX, val_x, y, val_y = train_test_split(train_x, train_y, test_size=0.1,\n                                      stratify = train_y, \n                                      random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(val_y[val_y==0])/len(val_y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(y[y==0])/len(y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipline for feature preparation and classification","metadata":{}},{"cell_type":"code","source":"embeddings_pipeline = Pipeline([\n        (\"word2vec\", w2v_spacy(nlp)),\n        (\"dim_reduce\", TruncatedSVD(50)),\n    ])\n\nfeature_prep = ColumnTransformer([\n                                ('numerical', ExtraAttributes(), ['text']),\n                                ('categorical', embeddings_pipeline, ['text'])\n])\n\nfull_pipeline = Pipeline([\n        ('feature_prep', feature_prep),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# full_pipeline.fit(X, y)\n# y_pred = full_pipeline.predict(val_x)\n# cr = classification_report(val_y, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creating pipeline for creating word embeddings for tweets, dimensional reduction and classification using Random Forest\n\nembeddings_pipeline_test = Pipeline([\n        (\"word2vec\", w2v_spacy(nlp)),\n        (\"dim_reduce\", TruncatedSVD(50)),\n        (\"classifier\", RandomForestClassifier(random_state=42)),\n    ]\n)\nembeddings_pipeline_test.fit(X, y)\ny_pred = embeddings_pipeline_test.predict(val_x)\ncr = classification_report(val_y, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = embeddings_pipeline_test.predict(X)\ntrain_cr = classification_report(y, y_pred)\ntrain_cr","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random search of hyper parameters with Xgboost classifier","metadata":{}},{"cell_type":"code","source":"\nparams = {\n    \"feature_prep__categorical__dim_reduce\": [\"passthrough\", TruncatedSVD(20), TruncatedSVD(50)],\n    \"classifier__max_depth\": [7], #, 11],\n    \"classifier__learning_rate\": [0.1], # [1, 0.1, 0.01],\n    \"classifier__n_estimators\": [50], #, 100],\n}\n\n\nfull_pipeline = Pipeline([\n        ('feature_prep', feature_prep),\n        (\"classifier\", xgb.XGBClassifier(use_label_encoder=False, eval_metric='auc')),\n    ])\n\n\n%timeit\nprint(\"Searching..\")\nmodel = RandomizedSearchCV(full_pipeline, param_distributions=params, \n                           scoring='f1', n_iter=5, cv=3, verbose=8)\nmodel.fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.best_score_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # refitting on entire training data using best params\n# model.refit\n\ny_pred = model.predict(val_x)\ncr = classification_report(val_y, y_pred)\nprint(cr)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.best_params_","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"confusion_matrix(val_y, y_pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summary:\n\n - Designed and implemented a pipeline to predict if a tweet is on a disaster\n - Data Cleaning:\n     - Removed hyperlinks, punctuations, special chars and numbers from the tweet\n     - Dropped id, location and keyword columns; as they were found to be not useful\n - Features used:\n      - Embeddings based on pretrained word 2 vec for the tweets. Experimented with small, medium and large word2vec models of spacy, where large provides the best results, however it is very memory intensive\n      - Truncated SVD used to reduce the dimensionality of the data\n      - Numerical features based on the average word length in a tweet and fraction of non stopwords\n - Pipelines created for numerical features and text based features\n - Columntransformer used to concatenate the features\n - A random forecast classifier was trained on the training data\n - Also, a xgboost classifier was trained with random hyper parameter search where feature engineering params can be tune with the params grid\n - Performance of the best classifier from random search is evaluated using the validation set (created with stratified split)\n - Recall of the positive class (disaster) is found to be low, specially in case of the small word 2 vec model\n - Random forest model trained without cross validation is overfitting the train set, and performs poorly on the validation set\n - Xgboost model trained with cross validation with the parameter grid selected is underfit on the training set and hence the performance can be futher improved by increasing the tree depth, max leaves or n_estimators.\n \n# Further improvements:\n\n- More features can be created by extracing hashtags, url text, emojis and sentiments from the tweet\n- Futher analysis of keyword column could allow for creation of a more useful feature based on it.\n- Character embeddings can be generated for special characters\n- Text can be vectorized with BERT which could possibly improve the performance by better capturing the contextual meaning in the tweet. Importance of embeddings is clear by the difference between small and large spacy word2vec models.\n- Scope for better tuning of the Xgboost model\n- A sequence based model like LSTM can be used which would learn the centextual meaning more accurately in the tweet","metadata":{}}]}